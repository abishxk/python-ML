# -*- coding: utf-8 -*-
"""network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1boNFMxA27YwsGXUhqskoBjW579tRy9pt
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Read the dataset
df_knn = pd.read_csv("train_dataset.csv")


# Update the selected features based on the actual column names in the dataset
selected_features = [
    'Port Number', 'Received Packets', 'Received Bytes', 'Sent Bytes',
    'Sent Packets', 'Port alive Duration (S)', 'Packets Rx Dropped',
    'Packets Tx Dropped', 'Packets Rx Errors', 'Packets Tx Errors',
    'Delta Received Packets', 'Delta Received Bytes', 'Delta Sent Bytes',
    'Delta Sent Packets', 'Delta Port alive Duration (S)',
    'Delta Packets Rx Dropped', 'Delta Packets Tx Errors',  # Adjusted feature names
    'Connection Point', 'Total Load/Rate', 'Total Load/Latest',
    'Unknown Load/Rate', 'Unknown Load/Latest', 'Latest bytes counter',
    'is_valid', 'Table ID', 'Active Flow Entries', 'Packets Looked Up',
    'Packets Matched', 'Max Size'
]

# Independent Variables (Features) and Dependent Variable (Target)
X_knn = df_knn[selected_features]
y_knn = df_knn['Label']

# Encode the target variable
label_encoder_knn = LabelEncoder()
y_knn = label_encoder_knn.fit_transform(y_knn)

# Split the dataset into training and testing sets
X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, y_knn, test_size=0.2, random_state=42)

# Standardize features
scaler_knn = StandardScaler()
X_train_knn = scaler_knn.fit_transform(X_train_knn)
X_test_knn = scaler_knn.transform(X_test_knn)

# Set the value of k for KNN
k_value_knn = 5
knn_classifier_knn = KNeighborsClassifier(n_neighbors=k_value_knn)

# Train the KNN classifier
knn_classifier_knn.fit(X_train_knn, y_train_knn)

# Make predictions on the test set
y_pred_knn = knn_classifier_knn.predict(X_test_knn)

# Evaluate the performance of the KNN classifier
accuracy_knn = accuracy_score(y_test_knn, y_pred_knn)
conf_matrix_knn = confusion_matrix(y_test_knn, y_pred_knn)
class_report_knn = classification_report(y_test_knn, y_pred_knn)

# Perform 10-fold cross-validation and print accuracies
accuracies_knn = cross_val_score(knn_classifier_knn, X_train_knn, y_train_knn, cv=10)
print("\nAccuracies for 10-fold cross-validation:")
for i, acc_knn in enumerate(accuracies_knn, 1):
    print("{:.2f}".format(acc_knn * 100))

# Print mean and standard deviation of accuracies
print("\nMean Accuracy: {:.2f}%".format(accuracies_knn.mean() * 100))
print("Standard Deviation: {:.2f}%".format(accuracies_knn.std() * 100))

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Read the dataset
df_xgb = pd.read_csv("train_dataset.csv")

# Update the selected features based on the actual column names in the dataset
selected_features_xgb = [
    'Port Number', 'Received Packets', 'Received Bytes', 'Sent Bytes',
    'Sent Packets', 'Port alive Duration (S)', 'Packets Rx Dropped',
    'Packets Tx Dropped', 'Packets Rx Errors', 'Packets Tx Errors',
    'Delta Received Packets', 'Delta Received Bytes', 'Delta Sent Bytes',
    'Delta Sent Packets', 'Delta Port alive Duration (S)',
    'Delta Packets Rx Dropped', ' Delta Packets Tx Dropped',
    'Delta Packets Rx Errors', 'Delta Packets Tx Errors',  # Adjusted feature names
    'Connection Point', 'Total Load/Rate', 'Total Load/Latest',
    'Unknown Load/Rate', 'Unknown Load/Latest', 'Latest bytes counter',
    'is_valid', 'Table ID', 'Active Flow Entries', 'Packets Looked Up',
    'Packets Matched', 'Max Size'
]

# Independent Variables (Features) and Dependent Variable (Target)
X_xgb = df_xgb[selected_features_xgb]
y_xgb = df_xgb['Label']

# Encode the target variable
label_encoder_xgb = LabelEncoder()
y_xgb = label_encoder_xgb.fit_transform(y_xgb)

# Split the dataset into training and testing sets
X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=0.2, random_state=42)

# Standardize features
scaler_xgb = StandardScaler()
X_train_xgb = scaler_xgb.fit_transform(X_train_xgb)
X_test_xgb = scaler_xgb.transform(X_test_xgb)

# Initialize the XGBoost classifier
xgb_classifier = XGBClassifier(random_state=42)

# Train the XGBoost classifier
xgb_classifier.fit(X_train_xgb, y_train_xgb)

# Make predictions on the test set
y_pred_xgb = xgb_classifier.predict(X_test_xgb)

# Evaluate the performance of the XGBoost classifier
accuracy_xgb = accuracy_score(y_test_xgb, y_pred_xgb)
conf_matrix_xgb = confusion_matrix(y_test_xgb, y_pred_xgb)
class_report_xgb = classification_report(y_test_xgb, y_pred_xgb)

# Perform 10-fold cross-validation and print accuracies
accuracies_xgb = cross_val_score(xgb_classifier, X_train_xgb, y_train_xgb, cv=10)
print("\nAccuracies for 10-fold cross-validation:")
for i, acc_xgb in enumerate(accuracies_xgb, 1):
    print("{:.2f}".format(acc_xgb * 100))

# Print mean and standard deviation of accuracies
print("\nMean Accuracy: {:.2f}%".format(accuracies_xgb.mean() * 100))
print("Standard Deviation: {:.2f}%".format(accuracies_xgb.std() * 100))

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Read the dataset
df_svm = pd.read_csv("train_dataset.csv")

# Update the selected features based on the actual column names in the dataset
selected_features_svm = [
    'Port Number', 'Received Packets', 'Received Bytes', 'Sent Bytes',
    'Sent Packets', 'Port alive Duration (S)', 'Packets Rx Dropped',
    'Packets Tx Dropped', 'Packets Rx Errors', 'Packets Tx Errors',
    'Delta Received Packets', 'Delta Received Bytes', 'Delta Sent Bytes',
    'Delta Sent Packets', 'Delta Port alive Duration (S)',
    'Delta Packets Rx Dropped', ' Delta Packets Tx Dropped',
    'Delta Packets Rx Errors', 'Delta Packets Tx Errors',  # Adjusted feature names
    'Connection Point', 'Total Load/Rate', 'Total Load/Latest',
    'Unknown Load/Rate', 'Unknown Load/Latest', 'Latest bytes counter',
    'is_valid', 'Table ID', 'Active Flow Entries', 'Packets Looked Up',
    'Packets Matched', 'Max Size'
]

# Independent Variables (Features) and Dependent Variable (Target)
X_svm = df_svm[selected_features_svm]
y_svm = df_svm['Label']

# Encode the target variable
label_encoder_svm = LabelEncoder()
y_svm = label_encoder_svm.fit_transform(y_svm)

# Split the dataset into training and testing sets
X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, test_size=0.2, random_state=42)

# Standardize features
scaler_svm = StandardScaler()
X_train_svm = scaler_svm.fit_transform(X_train_svm)
X_test_svm = scaler_svm.transform(X_test_svm)

# Initialize the Support Vector Machine classifier
svm_classifier = SVC(kernel='linear', random_state=42)

# Train the Support Vector Machine classifier
svm_classifier.fit(X_train_svm, y_train_svm)

# Make predictions on the test set
y_pred_svm = svm_classifier.predict(X_test_svm)

# Evaluate the performance of the Support Vector Machine classifier
accuracy_svm = accuracy_score(y_test_svm, y_pred_svm)
conf_matrix_svm = confusion_matrix(y_test_svm, y_pred_svm)
class_report_svm = classification_report(y_test_svm, y_pred_svm)

# Perform 10-fold cross-validation and print accuracies
accuracies_svm = cross_val_score(svm_classifier, X_train_svm, y_train_svm, cv=10)
print("\nAccuracies for 10-fold cross-validation:")
for i, acc_svm in enumerate(accuracies_svm, 1):
    print("{:.2f}".format(acc_svm * 100))

# Print mean and standard deviation of accuracies
print("\nMean Accuracy: {:.2f}%".format(accuracies_svm.mean() * 100))
print("Standard Deviation: {:.2f}%".format(accuracies_svm.std() * 100))

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Read the dataset
df_logreg = pd.read_csv("train_dataset.csv")

# Update the selected features based on the actual column names in the dataset
selected_features_logreg = [
    'Port Number', 'Received Packets', 'Received Bytes', 'Sent Bytes',
    'Sent Packets', 'Port alive Duration (S)', 'Packets Rx Dropped',
    'Packets Tx Dropped', 'Packets Rx Errors', 'Packets Tx Errors',
    'Delta Received Packets', 'Delta Received Bytes', 'Delta Sent Bytes',
    'Delta Sent Packets', 'Delta Port alive Duration (S)',
    'Delta Packets Rx Dropped', ' Delta Packets Tx Dropped',
    'Delta Packets Rx Errors', 'Delta Packets Tx Errors',  # Adjusted feature names
    'Connection Point', 'Total Load/Rate', 'Total Load/Latest',
    'Unknown Load/Rate', 'Unknown Load/Latest', 'Latest bytes counter',
    'is_valid', 'Table ID', 'Active Flow Entries', 'Packets Looked Up',
    'Packets Matched', 'Max Size'
]

# Independent Variables (Features) and Dependent Variable (Target)
X_logreg = df_logreg[selected_features_logreg]
y_logreg = df_logreg['Label']

# Encode the target variable
label_encoder_logreg = LabelEncoder()
y_logreg = label_encoder_logreg.fit_transform(y_logreg)

# Split the dataset into training and testing sets
X_train_logreg, X_test_logreg, y_train_logreg, y_test_logreg = train_test_split(X_logreg, y_logreg, test_size=0.2, random_state=42)

# Standardize features
scaler_logreg = StandardScaler()
X_train_logreg = scaler_logreg.fit_transform(X_train_logreg)
X_test_logreg = scaler_logreg.transform(X_test_logreg)

# Initialize the Logistic Regression classifier
logreg_classifier = LogisticRegression(random_state=42)

# Train the Logistic Regression classifier
logreg_classifier.fit(X_train_logreg, y_train_logreg)

# Make predictions on the test set
y_pred_logreg = logreg_classifier.predict(X_test_logreg)

# Evaluate the performance of the Logistic Regression classifier
accuracy_logreg = accuracy_score(y_test_logreg, y_pred_logreg)
conf_matrix_logreg = confusion_matrix(y_test_logreg, y_pred_logreg)
class_report_logreg = classification_report(y_test_logreg, y_pred_logreg)

# Perform 10-fold cross-validation and print accuracies
accuracies_logreg = cross_val_score(logreg_classifier, X_train_logreg, y_train_logreg, cv=10)
print("\nAccuracies for 10-fold cross-validation:")
for i, acc_logreg in enumerate(accuracies_logreg, 1):
    print("{:.2f}".format(acc_logreg * 100))

# Print mean and standard deviation of accuracies
print("\nMean Accuracy: {:.2f}%".format(accuracies_logreg.mean() * 100))
print("Standard Deviation: {:.2f}%".format(accuracies_logreg.std() * 100))

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Read the dataset
df_nb = pd.read_csv("train_dataset.csv")

# Update the selected features based on the actual column names in the dataset
selected_features_nb = [
    'Port Number', 'Received Packets', 'Received Bytes', 'Sent Bytes',
    'Sent Packets', 'Port alive Duration (S)', 'Packets Rx Dropped',
    'Packets Tx Dropped', 'Packets Rx Errors', 'Packets Tx Errors',
    'Delta Received Packets', 'Delta Received Bytes', 'Delta Sent Bytes',
    'Delta Sent Packets', 'Delta Port alive Duration (S)',
    'Delta Packets Rx Dropped', ' Delta Packets Tx Dropped',
    'Delta Packets Rx Errors', 'Delta Packets Tx Errors',
    'Connection Point', 'Total Load/Rate', 'Total Load/Latest',
    'Unknown Load/Rate', 'Unknown Load/Latest', 'Latest bytes counter',
    'is_valid', 'Table ID', 'Active Flow Entries', 'Packets Looked Up',
    'Packets Matched', 'Max Size'
]

# Independent Variables (Features) and Dependent Variable (Target)
X_nb = df_nb[selected_features_nb]
y_nb = df_nb['Label']

# Encode the target variable
label_encoder_nb = LabelEncoder()
y_nb = label_encoder_nb.fit_transform(y_nb)

# Split the dataset into training and testing sets
X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X_nb, y_nb, test_size=0.2, random_state=42)

# Standardize features (important for Naive Bayes)
scaler_nb = StandardScaler()
X_train_nb = scaler_nb.fit_transform(X_train_nb)
X_test_nb = scaler_nb.transform(X_test_nb)

# Create and train the Naive Bayes model
nb_classifier = GaussianNB()
nb_classifier.fit(X_train_nb, y_train_nb)

# Make predictions on the test set
y_pred_nb = nb_classifier.predict(X_test_nb)

# Calculate and print accuracy
accuracy_nb = accuracy_score(y_test_nb, y_pred_nb)
print("Accuracy for Naive Bayes: {:.2f}%".format(accuracy_nb * 100))

# Perform cross-validation and print accuracies one by one
accuracies_nb = cross_val_score(nb_classifier, X_train_nb, y_train_nb, cv=10)
print("\nAccuracies for 10-fold cross-validation:")
for i, accuracy in enumerate(accuracies_nb, 1):
    print("{:.2f}".format(accuracy * 100))

# Print mean and standard deviation of accuracies
print("\nMean Accuracy: {:.2f}%".format(accuracies_nb.mean() * 100))
print("Standard Deviation: {:.2f}%".format(accuracies_nb.std() * 100))