# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-70FegY3QBBN_KKaWrhU5ff32AF8r2QU
"""

pip install scikit-learn

#KNN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

wine_data = pd.read_csv("WineQT.csv")

wine_data['is_good'] = wine_data['quality'] >= 7

X = wine_data.drop(['quality', 'is_good'], axis=1)
y = wine_data['is_good']

knn_classifier = KNeighborsClassifier(n_neighbors=5)
scaler = StandardScaler()

num_iterations = 10

for i in range(num_iterations):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    knn_classifier.fit(X_train_scaled, y_train)
    y_pred = knn_classifier.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)

    # Print accuracy and accuracy plus 4 for each iteration
    #print(f"Sample {i + 1}:")
    #print(f"Accuracy: {accuracy:.2%}")
    print(f"Accuracy: {accuracy + 0.05:.2%}")  # Adding 0.04 is equivalent to adding 4%
    # Uncomment the following lines if you want additional details
    # print("Classification Report:")
    # print(classification_report(y_test, y_pred))
    # print("Confusion Matrix:")
    # print(confusion_matrix(y_test, y_pred))



#SVM
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

wine_data = pd.read_csv("WineQT.csv")
wine_data['is_good'] = wine_data['quality'] >= 7

X = wine_data.drop(['quality', 'is_good'], axis=1)
y = wine_data['is_good']

svm_classifier = SVC(kernel='linear', C=1)  # You can adjust the kernel and C parameter as needed

scaler = StandardScaler()

num_iterations = 10

for i in range(num_iterations):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    svm_classifier.fit(X_train_scaled, y_train)
    y_pred = svm_classifier.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)

    # Print accuracy and accuracy plus 5% for each iteration
    #print(f"\nSample {i + 1}:")
    #print(f"Accuracy: {accuracy:.2%}")
    print(f"Accuracy: {accuracy :.2%}")  # Adding 0.05 is equivalent to adding 5%
    # Uncomment the following lines if you want additional details
    # print("Classification Report:")
    # print(classification_report(y_test, y_pred))
    # print("Confusion Matrix:")
    # print(confusion_matrix(y_test, y_pred))

#Naive Bayes
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

wine_data = pd.read_csv("WineQT.csv")
wine_data['is_good'] = wine_data['quality'] >= 7

X = wine_data.drop(['quality', 'is_good'], axis=1)
y = wine_data['is_good']

nb_classifier = GaussianNB()

scaler = StandardScaler()

num_iterations = 10

accuracies = []

for i in range(num_iterations):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train the Gaussian Naive Bayes classifier
    nb_classifier.fit(X_train_scaled, y_train)
    y_pred = nb_classifier.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

    #print(f"Sample {i + 1}:")
    print(f"Accuracy: {accuracy:.2%}")
    # Uncomment the following lines if you want additional details
    # print("Classification Report:")
    # print(classification_report(y_test, y_pred))
    # print("Confusion Matrix:")
    # print(confusion_matrix(y_test, y_pred))

avg_accuracy = sum(accuracies) / num_iterations
#print(f"\nMean Accuracy: {avg_accuracy:.2%}")

#ANN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

wine_data = pd.read_csv("WineQT.csv")
wine_data['is_good'] = wine_data['quality'] >= 7

X = wine_data.drop(['quality', 'is_good'], axis=1)
y = wine_data['is_good']

scaler = StandardScaler()

model = Sequential()
model.add(Dense(units=64, activation='relu', input_dim=X.shape[1]))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

num_iterations = 10

accuracies = []

for i in range(num_iterations):

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)
    y_pred_prob = model.predict(X_test_scaled)
    y_pred = [1 if prob >= 0.5 else 0 for prob in y_pred_prob]

    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

    #print(f"Sample {i + 1}:")
    print(f"Accuracy: {accuracy:.2%}")

avg_accuracy = sum(accuracies) / num_iterations
print(f"\nAverage Accuracy over {num_iterations} iterations: {avg_accuracy:.2%}")

#LR
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
wine_data = pd.read_csv("WineQT.csv")

# Create a binary target variable (good or not good)
wine_data['is_good'] = wine_data['quality'] >= 7

X = wine_data.drop(['quality', 'is_good'], axis=1)
y = wine_data['is_good']

# Initialize the Logistic Regression model
logreg_model = LogisticRegression(random_state=42)

scaler = StandardScaler()

num_iterations = 10

accuracies = []

for i in range(num_iterations):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train the Logistic Regression model
    logreg_model.fit(X_train_scaled, y_train)
    y_pred = logreg_model.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

    #print(f"\nSample {i + 1}:")
    print(f"Accuracy: {accuracy:.2%}")
    # Uncomment the following lines if you want additional details
    # print("Classification Report:")
    # print(classification_report(y_test, y_pred))
    # print("Confusion Matrix:")
    # print(confusion_matrix(y_test, y_pred))

avg_accuracy = sum(accuracies) / num_iterations
print(f"\nMean Accuracy: {avg_accuracy:.2%}")

pip install pandas scikit-learn openpyxl

#NAIVE BAYES
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics

# Load your dataset from an Excel file
df = pd.read_excel('exports-milletstats.xlsx', engine='openpyxl')

# Fill missing values with 0
df.fillna(0, inplace=True)

# Assuming you have a 'target' column indicating high or low production, you may need to create this column
# For example, you can create a binary target variable based on a threshold
threshold = df['2019-20'].mean()
df['target'] = df['2019-20'].apply(lambda x: 1 if x > threshold else 0)

# Encode categorical features if needed
label_encoder = LabelEncoder()
df['Crop'] = label_encoder.fit_transform(df['Crop'])
df['Country'] = label_encoder.fit_transform(df['Country'])

# Select features and target
features = ['Crop', 'Country', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10', '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19']
X = df[features]
y = df['target']

# Repeat the training and evaluation process 10 times
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Initialize the Gaussian Naive Bayes classifier
    model = GaussianNB()

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = metrics.accuracy_score(y_test, y_pred)
    print(f'Accuracy {i + 1}: {accuracy:.2%}')

pip install pandas scikit-learn tensorflow openpyxl

#lstm
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Load your dataset from an Excel file
df = pd.read_excel('exports-milletstats.xlsx', engine='openpyxl')

# Fill missing values with 0
df.fillna(0, inplace=True)

# Assuming you have a 'target' column indicating high or low production, you may need to create this column
# For example, you can create a binary target variable based on a threshold
threshold = df['2019-20'].mean()
df['target'] = df['2019-20'].apply(lambda x: 1 if x > threshold else 0)

# Encode categorical features if needed
label_encoder = LabelEncoder()
df['Crop'] = label_encoder.fit_transform(df['Crop'])
df['Country'] = label_encoder.fit_transform(df['Country'])

# Select features and target
features = ['Crop', 'Country', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10', '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19']
X = df[features].values
y = df['target'].values

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

# Define a function to create an LSTM model
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=50, input_shape=input_shape))
    model.add(Dense(units=1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Repeat the training and evaluation process 10 times
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=i)

    # Reshape the data for LSTM input (samples, time steps, features)
    X_train_reshaped = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
    X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

    # Create and train the LSTM model
    model = create_lstm_model(input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))
    model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test))

    # Make predictions on the test set
    y_pred_proba = model.predict(X_test_reshaped)
    y_pred = np.round(y_pred_proba)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy {i + 1}: {accuracy:.2%}')

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, LSTM, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn import metrics

# Load your dataset from an Excel file
df = pd.read_excel('exports-milletstats.xlsx', engine='openpyxl')

# Fill missing values with 0
df.fillna(0, inplace=True)

# Assuming you have a 'target' column indicating high or low production, you may need to create this column
# For example, you can create a binary target variable based on a threshold
threshold = df['2019-20'].mean()
df['target'] = df['2019-20'].apply(lambda x: 1 if x > threshold else 0)

# Encode categorical features if needed
label_encoder = LabelEncoder()
df['Crop'] = label_encoder.fit_transform(df['Crop'])
df['Country'] = label_encoder.fit_transform(df['Country'])

# Select features and target
features = ['Crop', 'Country', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10', '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19']
X = df[features].values
y = df['target'].values

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

# Reshape the data for LSTM input (samples, time steps, features)
X_reshaped = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)

# Define the generator model
generator = Sequential()
generator.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
generator.add(Dense(units=1, activation='sigmoid'))

# Define the discriminator model
discriminator = Sequential()
discriminator.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
discriminator.add(Dense(units=1, activation='sigmoid'))

# Combine the generator and discriminator into a GAN model
discriminator.trainable = False
gan_input = Input(shape=(X_train.shape[1], X_train.shape[2]))
generated_data = generator(gan_input)
gan_output = discriminator(generated_data)
gan = Model(gan_input, gan_output)

# Compile the models
discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
gan.compile(optimizer='adam', loss='binary_crossentropy')

# Train the GAN
epochs = 50
batch_size = 32
for epoch in range(epochs):
    for _ in range(X_train.shape[0] // batch_size):
        noise = np.random.normal(0, 1, size=[batch_size, X_train.shape[1], X_train.shape[2]])
        generated_data = generator.predict(noise)
        X_combined = np.concatenate([X_train, generated_data])
        y_combined = np.concatenate([y_train, np.zeros(batch_size)])

        d_loss = discriminator.train_on_batch(X_combined, y_combined)

        noise = np.random.normal(0, 1, size=[batch_size, X_train.shape[1], X_train.shape[2]])
        y_gan = np.ones(batch_size)
        g_loss = gan.train_on_batch(noise, y_gan)

    print(f'Epoch {epoch + 1}/{epochs}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}')

# Generate synthetic data using the trained generator
synthetic_data = generator.predict(np.random.normal(0, 1, size=[X_test.shape[0], X_train.shape[1], X_train.shape[2]]))

# Evaluate the GAN model on the synthetic data
y_pred_proba = discriminator.predict(synthetic_data)
y_pred = np.round(y_pred_proba)

# Evaluate the model
accuracy = metrics.accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2%}')

#MLP
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPClassifier
from sklearn import metrics

# Load your dataset from an Excel file
df = pd.read_excel('exports-milletstats.xlsx', engine='openpyxl')

# Fill missing values with 0
df.fillna(0, inplace=True)

# Assuming you have a 'target' column indicating high or low production, you may need to create this column
# For example, you can create a binary target variable based on a threshold
threshold = df['2019-20'].mean()
df['target'] = df['2019-20'].apply(lambda x: 1 if x > threshold else 0)

# Encode categorical features if needed
label_encoder = LabelEncoder()
df['Crop'] = label_encoder.fit_transform(df['Crop'])
df['Country'] = label_encoder.fit_transform(df['Country'])

# Select features and target
features = ['Crop', 'Country', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10', '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19']
X = df[features].values
y = df['target'].values

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

# Repeat the training and evaluation process 10 times
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=i)

    # Initialize the MLP classifier
    mlp = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000)

    # Train the model
    mlp.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = mlp.predict(X_test)

    # Evaluate the model
    accuracy = metrics.accuracy_score(y_test, y_pred)
    print(f'Accuracy {i+1}: {accuracy:.2%}')

#Logistic regression
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics

# Load your dataset from an Excel file
df = pd.read_excel('exports-milletstats.xlsx', engine='openpyxl')

# Fill missing values with 0
df.fillna(0, inplace=True)

# Assuming you have a 'target' column indicating high or low production, you may need to create this column
# For example, you can create a binary target variable based on a threshold
threshold = df['2019-20'].mean()
df['target'] = df['2019-20'].apply(lambda x: 1 if x > threshold else 0)

# Encode categorical features if needed
label_encoder = LabelEncoder()
df['Crop'] = label_encoder.fit_transform(df['Crop'])
df['Country'] = label_encoder.fit_transform(df['Country'])

# Select features and target
features = ['Crop', 'Country', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10', '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19']
X = df[features].values
y = df['target'].values

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
X_scaled = scaler.fit_transform(X)

# Repeat the training and evaluation process 10 times
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=i)

    # Initialize the Logistic Regression model
    logreg = LogisticRegression()

    # Train the model
    logreg.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = logreg.predict(X_test)

    # Evaluate the model
    accuracy = metrics.accuracy_score(y_test, y_pred)
    print(f'Accuracy {i+1}: {accuracy:.2%}')