# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19EmBcuxeuXnASLvd1t8a8Xijrq1-ekhG
"""

#Random Forest
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('spam.csv', encoding='latin-1')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=42)

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Encode the 'v1' labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Train a Random Forest classifier
random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest_classifier.fit(X_train_counts, y_train_encoded)

# Make predictions on the test set
y_pred = random_forest_classifier.predict(X_test_counts)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f'Accuracy: {accuracy:.3f}')

print('\nClassification Report:')
print(classification_report(y_test_encoded, y_pred))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test_encoded, y_pred))

#KNN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=42)

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Encode the 'v1' labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Train a K-Nearest Neighbors classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train_counts, y_train_encoded)

# Make predictions on the test set
y_pred = knn_classifier.predict(X_test_counts)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f'Accuracy: {accuracy:.3f}')

print('\nClassification Report:')
print(classification_report(y_test_encoded, y_pred))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test_encoded, y_pred))

#support vector machine
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=42)

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Encode the 'v1' labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Train a Support Vector Machine classifier
svm_classifier = SVC(kernel='linear', C=1.0)
svm_classifier.fit(X_train_counts, y_train_encoded)

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test_counts)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f'Accuracy: {accuracy:.3f}')

print('\nClassification Report:')
print(classification_report(y_test_encoded, y_pred))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test_encoded, y_pred))

#polynomial regression
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=42)

# Encode the 'v1' labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Create a pipeline with Polynomial Regression
degree = 2  # Set the degree of the polynomial

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

polyreg_classifier = make_pipeline(PolynomialFeatures(degree), LinearRegression())

# Fit the model
polyreg_classifier.fit(X_train_counts, y_train_encoded)

# Make predictions on the test set
y_pred = polyreg_classifier.predict(X_test_counts)

# Convert predicted values to binary labels (e.g., spam or ham)
y_pred_binary = (y_pred > 0.5).astype(int)

# Evaluate the model (Note: Polynomial regression might not be suitable for classification tasks)
accuracy = accuracy_score(y_test_encoded, y_pred_binary)
print(f'Accuracy: {accuracy:.3f}')

print('\nClassification Report:')
print(classification_report(y_test_encoded, y_pred_binary))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test_encoded, y_pred_binary))

#Naive Bayes
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=42)

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

# Encode the 'v1' labels
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Train a Multinomial Naive Bayes classifier
naive_bayes_classifier = MultinomialNB()
naive_bayes_classifier.fit(X_train_counts, y_train_encoded)

# Make predictions on the test set
y_pred = naive_bayes_classifier.predict(X_test_counts)

# Evaluate the model
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f'Accuracy: {accuracy:.3f}')

print('\nClassification Report:')
print(classification_report(y_test_encoded, y_pred))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test_encoded, y_pred))

#10 samples Random forest
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Encode the 'v1' labels
label_encoder = LabelEncoder()
df['v1'] = label_encoder.fit_transform(df['v1'])

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['v2'])
y = df['v1']

# Print 10 accuracies for 10 random samples
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Train a Random Forest classifier
    random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
    random_forest_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = random_forest_classifier.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for Sample {i + 1}: {accuracy:.4f}')

    # Optionally, you can print other evaluation metrics
    # print('\nClassification Report:')
    # print(classification_report(y_test, y_pred))

    # print('\nConfusion Matrix:')
    # print(confusion_matrix(y_test, y_pred))

#10 samples KNN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Encode the 'v1' labels
label_encoder = LabelEncoder()
df['v1'] = label_encoder.fit_transform(df['v1'])

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['v2'])
y = df['v1']

# Print 10 accuracies for 10 random samples
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Train a K-Nearest Neighbors classifier
    knn_classifier = KNeighborsClassifier(n_neighbors=5)
    knn_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = knn_classifier.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for Sample {i + 1}: {accuracy:.4f}%')

    # Optionally, you can print other evaluation metrics
    # print('\nClassification Report:')
    # print(classification_report(y_test, y_pred))

    # print('\nConfusion Matrix:')
    # print(confusion_matrix(y_test, y_pred))

#10 samples SVM
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Encode the 'v1' labels
label_encoder = LabelEncoder()
df['v1'] = label_encoder.fit_transform(df['v1'])

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['v2'])
y = df['v1']

# Print 10 accuracies for 10 random samples
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Train a Support Vector Machine classifier
    svm_classifier = SVC(kernel='linear', C=1.0)
    svm_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = svm_classifier.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for Sample {i + 1}: {accuracy:.4f}')

    # Optionally, you can print other evaluation metrics
    # print('\nClassification Report:')
    # print(classification_report(y_test, y_pred))

    # print('\nConfusion Matrix:')
    # print(confusion_matrix(y_test, y_pred))

#10 samples polynomial regression
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Encode the 'v1' labels
label_encoder = LabelEncoder()
df['v1'] = label_encoder.fit_transform(df['v1'])

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['v2'])
y = df['v1']

# Print 10 accuracies for 10 random samples
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Create a pipeline with Polynomial Regression
    degree = 2  # Set the degree of the polynomial
    polyreg_classifier = make_pipeline(PolynomialFeatures(degree), LinearRegression())

    # Fit the model
    polyreg_classifier.fit(X_train.toarray(), y_train)

    # Make predictions on the test set
    y_pred = polyreg_classifier.predict(X_test.toarray())

    # Convert predicted values to binary labels
    y_pred_binary = (y_pred > 0.5).astype(int)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred_binary)
    print(f'Accuracy for Sample {i + 1}: {accuracy:.4f}%')

    # Optionally, you can print other evaluation metrics
    # print('\nClassification Report:')
    # print(classification_report(y_test, y_pred_binary))

    # print('\nConfusion Matrix:')
    # print(confusion_matrix(y_test, y_pred_binary))

#10 samples naive bayes
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Load the dataset with specified encoding
df = pd.read_csv('spam.csv', encoding='latin-1')

# Encode the 'v1' labels
label_encoder = LabelEncoder()
df['v1'] = label_encoder.fit_transform(df['v1'])

# Use CountVectorizer to convert text data into numerical features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['v2'])
y = df['v1']

# Print 10 accuracies for 10 random samples
for i in range(10):
    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)

    # Train a Multinomial Naive Bayes classifier
    naive_bayes_classifier = MultinomialNB()
    naive_bayes_classifier.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = naive_bayes_classifier.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy for Sample {i + 1}: {accuracy:.4f}')

    # Optionally, you can print other evaluation metrics
    # print('\nClassification Report:')
    # print(classification_report(y_test, y_pred))

    # print('\nConfusion Matrix:')
    # print(confusion_matrix(y_test, y_pred))